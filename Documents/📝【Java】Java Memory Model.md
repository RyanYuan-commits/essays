---
type: Java 基础
finished: "false"
---
## 1 引言: Java 并发编程中的挑战与 JMM 核心作用

### 1.1 程序执行的非预期性

```java
static int counter = 0;  
  
public static void main(String[] args) throws InterruptedException {  
    Thread thread1 = new Thread(() -> counter = 1, "thread1");  
  
    Thread thread2 = new Thread(() -> {  
        while (counter == 0) {  
            // do nothing  
        }  
    }, "thread2");  
  
    thread2.start();  
    Thread.sleep(1000);  
    thread1.start();  
}
```

来看一个案例, `thread1` 启动时会改变计数器的值, `thread2` 等待计数器变为非零后停止.

按照我们的预期, 程序将会很快终止, 但运行后发现程序没有退出...

使用 `jstack` 打印下线程运行状态:

```java
"thread2" #16 prio=5 os_prio=0 cpu=30593.75ms elapsed=32.08s tid=0x000002009d231020 nid=0x5bf0 runnable  [0x0000007d53cfe000]
   java.lang.Thread.State: RUNNABLE
        at com.ryan.JavaMemoryModelDemo.lambda$main$1(JavaMemoryModelDemo.java:17)
        at com.ryan.JavaMemoryModelDemo$$Lambda$2/0x000002009f000c18.run(Unknown Source)
        at java.lang.Thread.run(java.base@17.0.16/Thread.java:840)
```

可以看到 `thread2` 并未成功退出.

### 1.2 原因浅析

现代的 CPU 一般是**多级缓存架构**, 为了平衡 CPU 和内存之间的速度差异, 会设置多级高速缓存 Cache;

核心在操作内存的时候, 会先将内容读取到缓存中, 操作完成后再将其刷回主存.

上面的案例中, `thread2` 读取的是缓存中的值, 所以一直没有成功退出.

### 1.3 JMM 的定义与定位

Java 内存模型 (JMM), 是 Java 虚拟机定义的一套规范, 旨在**屏蔽底层硬件的差异**, 为 Java 程序提供一个统一的内存访问规则.

上面提到的多级缓存架构, 只是一个规范, 不同厂商实现的 CPU 硬件的指令集可能会不同;

JMM 是一个**抽象模型**, 其在 CPU 多级缓存架构的基础上, 抽象出了一个统一的内存模型, 作为开发者, 可以不必关心底层架构, 只需遵循 JMM 定义的规范即可.

在上面的案例中, 由于 CPU 缓存的存在, `thread2` 没有进行缓存的同步, 导致它读取的一直是 `counter` 的旧值, `thread2` "看不到" `thread1` 所做的修改.

这就是一个典型的**可见性**问题, JMM 也提供了一系列明确的规则, 来避免这类非预期的情况发生.

## 2 JMM 理论基础-问题溯源

### 2.1 硬件层面的优化与挑战 - 多级缓存

#### 现代多级缓存架构

![[CPU 多级缓存模型.png|500]]

上面也提到处理器的处理速度很快, 内存处理速度远远赶不上处理器的处理速度;

为了解决 CPU 处理速度和内存处理速度不对等的问题, 我们引入了CPU Cache, 现代的 CPU Cache 一般分为三层, 即 L1、L2、L3 Cache;

当我们处理器需要处理某份数据时，这份数据首先会把数据从内存读入到缓存中, 然后交由处理器处理, 处理器处理完成之后再将数据写回缓存, 最后写回内存.

#### 理论支持

这种设计的理论支持是速度匹配和局部性原理:

- **速度匹配**: 在工程学中, 解决速度不适配主要有两种方式, 速度适配和空间缓存, CPU 多级缓存就是常见的空间缓存, 速度适配最常见的案例就是多级齿轮.
- **局部性原理**: 时间局部性, 如果一个信息被访问, 那么大概率这个信息会再次被访问(LRU, LFU 算法), 空间局部性, 如果一个信息被访问, 那大概率这个信息附近的数据也会被访问(缓存行).

#### 带来的挑战

多级缓存的设计为并发编程带来了**可见性**保障上的挑战.

且 CPU 多级缓存仅是一个概念, 不同厂商的架构和指令集设计均不同, 如果不抽象这种差异, 就无法做到 "一次编写, 到处运行".

### 2.2 硬件层面的优化与挑战 - 指令重排序

#### 指令重排序

为了提升速度和性能, 计算机在执行代码的时候, 会对指令进行重排序, 也就是在执行代码的时候, 不一定是按照程序的代码顺序依次执行.

```java
int a = 1; // 1
int b = 2; // 2
int c = a + b;
```

比如上面的代码中, 调换语句 1, 2 的顺序不影响最终结果, 因此可以进行重排.

#### 常见的指令重排

常见的指令重排序有:

- **编译器优化重排**: 例如 JVM, JIT 编译器, 在不改变单线程语义的情况下, 重新安排指令的执行顺序.
- **指令并行重排**: 现代处理器采用了指令级并行技术 (Instruction-Level Parallelism, ILP) 来将多条指令重叠执行. 如果不存在数据依赖性, 处理器可以改变语句对应机器指令的执行顺序.
- **多级缓存导致的指令重排**: 多线程中当线程 A 修改元素 a 但未写回主存, 线程 B 读取元素 a 时, 对于线程 B 来说上一句指令似乎没有执行, 类似发生指令重排.

#### 带来的挑战

指令重排序可以保证单线程串行语义一致, 但是**没有义务保证在多线程之间语义一致**, 在并发条件下, 指令重排序可能会导致**有序性**的问题.

为了防止指令重排带来的影响, 在并发编程的条件下, 我们可能需要考虑禁止指令重排序;

编译器和处理器的指令重排序的禁止方式不一样:

- 对于编译器, 通过禁止特定类型的编译器重排序的方式来禁止重排序;
- 对于处理器, 通过插入内存屏障 (Memory Barrier, 或有时叫做内存栅栏，Memory Fence) 的方式来禁止特定类型的处理器重排序.

### 2.3 操作分解与线程调度

#### 原子性问题

像 Java 这样的高级语言, 其一条语句可能会分解成多个独立的的机器指令;

以最常见的 `i++` 操作为例, 在高级语言中它是一个单一的语句. 但在底层, 它通常被分解为三个步骤:

- 读取: 将 i 值从主内存或者工作内存读取到寄存器中;
- 修改: 在寄存器对 i 进行加一的操作;
- 写入: 将新值写回主内存或者工作内存.

#### 带来的挑战

在并发条件下, 多个线程执行 `i++` 操作, 可能会导致一个状态被另一个线程的中间状态覆盖, 最终导致数据的不一致或者丢失, 所以在某些场景下, 必须保证**原子性**.

### 2.4 并发编程的三大核心特性

上面三个小节, 提到了原子性, 可见性, 顺序性三个特性, 为了确保程序在多线程环境下的正确性, 我们必须确保以下三个关键特性得到满足.

- **原子性**: 一个操作是**不可分割**的. 在执行过程中, 它要么全部完成, 要么完全不执行, 不存在被其他线程中断的可能.
- **可见性**: 一个线程对共享变量所做的修改, 对其他线程是**立即可见**的.
- **顺序性**: 程序的执行顺序按照代码的逻辑顺序执行, 不会因为编译器或处理器的优化而改变.

## 3 JMM 模型 - 抽象规范

在上一节中, 提到了并发编程带来的各种挑战; 为了应对这些挑战, JMM 应运而生, 与 CPU 多级缓存架构不同, 它不是一个物理实现, 而是一种 **抽象规范**.

### 3.1 屏蔽底层差异

#### 线程和主内存的抽象关系

在 2.1 小节, 我们提到 CPU 多级缓存是现在处理器的一种普遍的硬件架构设计, 然而, 不同厂商的 CPU 在底层架构和指令集上可能存在差异, 这会导致在不同硬件平台上, 程序的行为可能不一致. 因此，Java 内存模型必须提供一层抽象来屏蔽这些底层差异, 确保 Java 程序在任何硬件平台上都能保持一致的并发行为.

![[工作内存与主内存.png|600]]


JMM 定义了线程和主内存之间的抽象关系: 线程之间的共享变量存储在主内存中, 每个线程都有一个私有的工作内存;

工作内存中存储了该线程以读/写共享变量的副本;

本地内存是 JMM 的一个抽象概念, 并不真实存在; 它涵盖了缓存, 写缓冲区, 寄存器以及其他的硬件和编译器优化.

这样, 无论是编写, 编译还是解释代码, 开发者都只需要基于 JMM 定义的这个抽象架构进行思考.

在代码实际运行时, JMM 也会借助这个抽象层来屏蔽底层硬件实现的差异, 从而真正实现 Java 的 "一次编写, 到处运行" 的承诺.

#### JMM 的八大逻辑动作

上面的部分是对 CPU 多级架构的抽象, 除此之外, JMM 还需要抽象出一套逻辑操作, 来实际的完成各种复杂的内存操作.

这些逻辑动作用于描述共享变量在主内存与工作内存之间的可见性与顺序性规则;

这些逻辑动作随后由 JVM 直接映射为底层 CPU 的 load、store、fence、atomic 指令序列, 或在运行时插入特定内存屏障, 从而在不同硬件平台上实现统一且可预期的并发语义.

具体来说, 主要有以下的八大逻辑动作:

| 操作     | 作用                           | 数据来源             | 数据去向     |
| ------ | ---------------------------- | ---------------- | -------- |
| lock   | 将主内存中的共享变量标记为某一线程独占的锁定状态     | —                | 主内存共享变量  |
| unlock | 将处于锁定状态的共享变量解除锁定，使其他线程可再次访问  | —                | 主内存共享变量  |
| read   | 把主内存中的共享变量值传输到工作内存           | 主内存共享变量          | 工作内存     |
| load   | 把刚刚传输到工作内存的值赋给工作内存中的变量副本     | 工作内存(read 得到的数据) | 工作内存变量副本 |
| use    | 把工作内存变量副本的值提供给 CPU 执行引擎运算    | 工作内存变量副本         | CPU 执行引擎 |
| assign | 把 CPU 执行引擎计算后的结果重新写回工作内存变量副本 | CPU 执行引擎         | 工作内存变量副本 |
| store  | 把工作内存中修改后的变量值传回主内存           | 工作内存变量副本         | 主内存      |
| write  | 将传回主内存的值正式写回到主内存的共享变量        | 主内存(store 得到的数据) | 主内存共享变量  |

### 3.2 解决三大问题

JMM 不仅仅是一个抽象的内存模型, 它更是一套严谨的规则集合, 旨在通过定义特定的内存语义来解决并发编程中的可见性, 有序性和原子性问题.

#### 解决可见性问题

JMM 通过规定 `read`, `load`, `store`, `write` 这四个操作的组合 **顺序和时机** 来保证可见性;

当一个线程修改了共享变量时, JMM 会确保其工作内存中的修改 (通过 `assign` 操作) 能够及时地通过 `store` 和 `write` 操作刷新到主内存. 

同时, 当其他线程需要读取这个共享变量时, JMM 会 **强制** 它们通过 `read` 和 
 `load` 操作从 **主内存** 中获取最新值, 而不是使用其工作内存中的旧副本. 通过这种强制性的同步机制, JMM 确保了线程之间对共享变量修改的可见性.

#### 解决有序性问题

JMM 通过**对八大逻辑动作的执行顺序施加限制**来解决有序性问题, 它定义了哪些操作之间是不能被重排序的, 即使底层硬件或编译器试图进行优化;

特别是 `lock` 和 `unlock` 操作, 它们在 JMM 中扮演着“内存屏障”的角色. 

- 当一个线程执行 `lock` 操作时, 它会确保在此之前的内存操作都已完成并对其他线程可见; 
- 当执行 `unlock` 操作时, 它会确保在此之前的内存操作都已刷新到主内存. 

这些操作的约束, 使得 JMM 能够保证在并发环境下, 关键操作的执行顺序能够符合程序员的预期, 避免了因重排序导致的逻辑错误.

#### 解决原子性问题

JMM 主要通过 `lock` 和 `unlock` 这两个操作来解决原子性问题. 当一个线程对共享变量执行 `lock` 操作时, 它就获得了对该变量的独占访问权, 其他线程无法同时对该变量进行 `lock` 操作. 

在 `lock` 和 `unlock` 之间的所有操作 (包括 `read`、`load`、`use`、`assign`、`store`、`write` 等) 都将被视为一个不可分割的原子单元. 

这意味着在 `lock` 和 `unlock` 期间, 该线程对共享变量的所有操作都不会被其他线程干扰, 从而保证了操作的完整性和数据的一致性. 

## 4 JMM 模型 - Happens-Before

### 4.1 为什么需要 Happens-Before?

Happens-Before 原则的提出, 是为了调和两个根本性的冲突:

- **程序员的“理想世界”**: 程序员倾向于以**顺序一致性**的视角来理解程序. 我们期望代码是严格按照书写顺序执行的, 并且一个线程的修改能立即被其他线程看到. 这是一种简单直观但性能低下的模型.
- **硬件与编译器的“现实世界”**: 为了追求极致性能, 底层硬件（CPU）和编译器会大量使用重排序和缓存优化. 它们的目标是**只要单线程执行结果不变，就允许任意优化**.

但是, 如果不对这种“现实世界”施加约束, 多线程程序的行为将完全不可预测, 因此, JMM 设计了 Happens-Before 原则, 它在两者之间建立了一座桥梁;

它既**允许**了大部分的性能优化, 又通过定义明确的规则**禁止**那些会破坏多线程正确性的优化. Happens-Before 不代表时间的先后顺序, 而是一种**偏序关系**, 用于约束编译器和处理器, 并为程序员提供同步保证;

其设计思想是: 在保证程序正确性的前提下, 尽可能允许编译器和处理器进行性能优化.

### 4.2 Happens-Before 的核心规则

Happens-Before 规定了以下核心规则:

- **程序顺序规则**: 在一个线程内, 书写在前面的操作 Happens-Before 于书写在后面的操作.
- **监视器锁规则**: 对一个锁的 `unlock` 操作 Happens-Before 于后续对同一个锁的 `lock` 操作.
- **`volatile` 变量规则**: 对一个 `volatile` 变量的写操作 Happens-Before 于后续对这个变量的读操作.
- **传递性**: 如果操作 A Happens-Before 操作 B, 并且操作 B Happens-Before 操作 C, 那么操作 A Happens-Before 操作 C.

在满足 Happens-Before 定义的各种规则的前提下, 允许编译器和处理器进行性能优化.

## 5 借助 JMM 编写代码

上面提到的都是 JMM 的逻辑指令, 但是在我们日常代码中并不需要直接操作这些逻辑指令, JMM 为我们提供了多种工具.

### 5.1 volatile 关键字

`volatile` (adj. 易变的), 主要用于保证可见性和有序性.

上面的 `volatile` 规则中提到: 对一个 `volatile` 变量的写操作 Happens-Before 于后续对这个变量的读操作.

当一个变量被 `volatile` 修饰时, 对其的读写操作都会在底层插入内存屏障. 内存屏障类似指令的一个切面, 可以执行一些额外逻辑:

- Load Barrier 读屏障
	- **作用**: 在执行读操作时, 它会确保当前线程的工作内存中该变量的副本失效，**强制从主内存中读取最新数据**; 
	- **有序性保证**: 同时, 它会阻止该 `volatile` 读操作**之后**的任何读操作被重排序到该 `volatile` 读操作**之前**.
- Store Barrier 写屏障
	- **作用**: 在执行 `volatile` 写操作时, 它会**强制将该写操作的结果立即刷新到主内存**.
	- **有序性保证**: 它会阻止该 `volatile` 写操作**之前**的任何写操作被重排序到该 `volatile` 写操作**之后**, 确保所有先前的修改都已对其他线程可见.

### 5.2 synchronized 关键字

#### 作用

同时保障**原子性**, **可见性**和**有序性**.

原理: 通过独占式锁机制, 确保同一时间只有一个线程可以进入同步代码块. 

在锁释放时, 它会强制将该线程工作内存中的所有修改刷新到主内存; 而在获取锁时，它会强制清空当前线程的缓存，从主内存重新读取变量.

#### 底层实现原理

```java
public class Main {  
    static int a = 0;  
    public static void main(String[] args) throws InterruptedException {  
        synchronized (Main.class) {  
            a += 1;  
        }  
    }  
}

// 编译后的字节码

 0 ldc #2 <com/ryan/Main>
 2 dup
 3 astore_1
 4 monitorenter
 5 getstatic #3 <com/ryan/Main.a : I>
 8 iconst_1
 9 iadd
10 putstatic #3 <com/ryan/Main.a : I>
13 aload_1
14 monitorexit
15 goto 23 (+8)
18 astore_2
19 aload_1
20 monitorexit
21 aload_2
22 athrow
23 return
```

关注两条指令: `monitorenter` 和 `monitorexit`, 其中 `monitorenter` 指令指向同步代码块的开始位置, `monitorexit` 指令则指明同步代码块的结束位置.

`synchronized` 底层中, 多个线程之间的同步是通过 `monitor` 结构实现的:

![[synchronized架构.png|500]]


上图展示是 `monitor` 的基本结构，它由这三部分构成：

- `Entry Set`: 等待获取对象锁的线程. 
- `The Owner`: 获取到锁的线程 (只能有一个), 获取到锁的线程可以通过 `notify` 等方法来唤醒 Wait Set 中的线程.
- `Wait Set`: 等待的线程, 是在 `synchronized` 代码块中调用 `wait` 方法的线程.

但是, `synchronized` 的 monitor 实现依赖的是操作系统级别的 mutex lock, 使用时会导致用户态和内核态的切换, 带来一些的性能开销.

#### 锁升级机制

`synchronized` 关键字的实现依赖于对象头中的 Mark Word 来存储锁信息. 

为了在不同竞争程度下提供最佳性能, JVM 引入了锁升级 (Lock Escalation) 机制, 将锁分为以下四种状态, 它们会随着竞争的加剧而逐级升级:

![[对象头中对锁状态的存储.png|700]]

**无锁状态**: 此时对象没有被任何线程锁定, 所有线程都可以自由访问资源, 不涉及任何同步开销.

使用的场景为:

- 变量本身不会在多线程环境下被共享或修改.
- 变量虽然在多线程环境下, 但其操作本身是原子性的 (如对 `int` 类型的简单赋值), 或者业务逻辑上不需要严格的同步保护.
- 开发者选择使用其他并发机制 (如 `java.util.concurrent.atomic` 包中的原子类，即“无锁编程”) 来管理共享变量, 而不是 `synchronized`.


**偏向锁**: 当一个对象被首次加锁时, 如果只有一个线程反复获取该锁, JVM 会将该锁“偏向”于这个线程. 对象头中会存储该线程的ID.

基本原理为: 当偏向线程再次尝试获取锁时, 只需检查对象头中的线程 ID 是否与自身 ID 匹配. 如果匹配, 无需进行任何同步操作, 即可直接进入临界区,. 极大地降低了锁开销.


**轻量级锁**: 当偏向锁遇到竞争 (即有其他线程尝试获取该锁)时. 偏向锁会升级为轻量级锁. 此时, 对象头中存储的是指向线程栈帧中 **Lock Record** 的指针.

每个常识获取锁的线程会在自己的栈帧中创建一个 Lock Record, 线程通过 CAS 操作尝试将对象头中的 Mark Word 替换为指向自身的 lock Record 的指针;

如果 CAS 成功, 标识获取锁成功, 线程会将对象头原始的 Mark Word 复制到自己的 Lock Record 中.

如果 CAS 失败, 表示有其他线程正在竞争锁, 此时, 未获取到锁的线程不会立即阻塞, 而是会进行自旋, 也就是在不放弃 CPU 的情况下, 反复尝试 CAS 操作.


**重量级锁**: 当轻量级锁的自旋尝试达到一定次数仍未成功, 或者有大量线程竞争锁时, 锁会升级为重量级锁.

此时, 未获取到锁的线程会被阻塞 (挂起), 并进入等待队列, 直到持有锁的线程释放锁. 这涉及到操作系统层面的线程上下文切换, 开销较大.