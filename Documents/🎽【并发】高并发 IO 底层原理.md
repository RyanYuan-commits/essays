## 1 高并发 IO 底层原理

### 1.1 IO 读写基础原理

为了避免用户直接操操作内核，保证内核安全，操作系统将内存（虚拟内存）划分为内核空间（Kernel-Space）和用户空间（User-Space）。
操作系统的核心是内核，相比于普通的应用程序，内核既可以访问内核空间，也可以访问底层硬件。

进程处于用户态的时候是不能访问内核空间的数据的，因此要进行系统调用的时候，必须要将进程切换到内核态才可以；
用户进程需要通过系统调才能向内核发出指令，完成调用系统资源等操作。

进程进行 IO 读写，依赖于底层的 IO 读写，基本上会用到两种系统调用 `sys_read` 和 `sys_write`，这两个系统调用都不会直接操控物理设备，而是会涉及到 **缓冲区**。
- 上层引用调用 `sys_read`，实际执行的操作是内核将数据复制到应用程序的缓冲区。
- 应用调用 `sys_write`，实际执行的操作是将用户缓冲区的数据复制到内核缓冲区。
简单来说，应用程序的 IO 操作，执行的不是设备级别的读写，而是缓存的复制。

`sys_read` 和 `sys_write` 都不负责内核缓冲区和底层（网卡、磁盘等）硬件设备的交换，这些底层的读写操作是由内核来完成的。
### 1.2 内核缓冲区和进程缓冲区

内核缓冲区和应用缓冲区在数量上也有所不同，在 Linux 中，操作系统的内核只有一个内核缓冲区，而每个应用又有自己独立的缓冲区；
Linux 下用户程序的 IO 读写程序，在大多数情况下，并没有进行实际的 IO 操作，而是在用户缓冲区和内核缓冲区之间的数据交换。

## 2 五种主要的 IO 模型

### 2.1 同步与异步、阻塞与非阻塞

==同步与异步可以看成发起 IO 请求的两种方式==
- 同步是应用程序是主动发起的 IO 请求的一方，系统内核是被接收的一方；
- 而异步是操作系统发起的 IO 请求的一方，用户是被动接收方。
==阻塞与非阻塞==
- 阻塞 IO，指的是需要等待内核 IO 操作完全完成后，才返回到用户空间执行用户程序的操作指令，阻塞指的是发起 IO 请求的进程或者线程的执行状态是阻塞的。
- 而不需要等待内核 IO 操作完成即可以返回用户空间继续执行的就是非阻塞的 IO。
### 2.2 同步阻塞 IO（Blocking IO）

默认情况下，在 Java 程序中，所有对 Socket 连接进行的 IO 操作都是同步阻塞 IO。
![[同步阻塞 IO 图例.png]]
调用流程大致如下：
- Java 进行 IO 读后，发起 `sys_read` 系统调用，用户线程就进入了阻塞状态。
- 内核收到调用后，就开始准备数据，如果数据还未完整到达内核缓冲区（例如还没有收到一个完整的 socket 包），此时内核就需要等待。
- 内核等待数据到达完成后，将数据复制到用户缓冲区，然后内核返回结果（例如返回复制到用户缓冲区的字节数）。
- 直到内核返回用户线程才结束阻塞状态，重新运行。

### 2.3 同步非阻塞 IO（None Blocking IO）

在 Linux 系统下，socket 连接默认是阻塞模式，可以通过设置将 socket 变成为非阻塞的模式（Non-Blocking）。在NIO模型中，应用程序一旦开始 IO 系统调用，会出现以下两种情况：
- 在内核缓冲区中没有数据的情况下，系统调用会立即返回，返回一个调用失败的信息。
- 在内核缓冲区中有数据的情况下，在数据的复制过程中系统调用是阻塞的，直到
完成数据从内核缓冲复制到用户缓冲。复制完成后，系统调用返回成功，用户进程（或者线程）可以开始处理用户空间的缓存数据。
![[同步非阻塞 IO 图例.png]]
- 在内核数据没有准备好的阶段，用户线程发起IO请求时，立即返回。所以，为了读取到最终的数据，用户进程（或者线程）需要不断地发起IO系统调用。
- 内核数据到达后，用户进程（或者线程）发起系统调用，用户进程（或者线程）**阻塞**。内核开始复制数据，它会将数据从内核缓冲区复制到用户缓冲区，然后内核返回结果（例如返回复制到的用户缓冲区的字节数）。
- 用户进程（或者线程）在读数据时，没有数据会立即返回而不阻塞，用户空间需要经过多次的尝试，才能保证最终真正读到数据，而后继续执行。

应用程序的线程需要不断地进行IO系统调用，轮询数据是否已经准备好，如果没有准备好，就继续轮询，直到完成IO系统调用为止。
每次发起的IO系统调用，在内核等待数据过程中可以立即返回。用户线程不会阻塞，实时性较好。
但是 NIO 需要应用程序不断地轮询内核，这将占用大量的CPU时间，效率低下。
总体来说，在高并发应用场景中，同步非阻塞IO是性能很低的，也是基本不可用的，一般Web服务器都不使用这种IO模型。在Java的实际开发中，也不会涉及这种IO模型。
但是此模型还是有价值的，其作用在于，其他IO模型中可以使用非阻塞IO模型作为基础，以实现其高性能。
**注意：NIO 和 Java 的 NIO 不是一个东西，Java 中对 NIO 是 New IO 的简称，其所属的模型是 IO 多路复用模型。**

### 2.4 IO 多路复用模型（IO Multiplexing）

如何避免同步非阻塞IO模型中轮询等待的问题呢？这就是IO多路复用模型。
在IO多路复用模型中，引入了一种新的系统调用，查询IO的就绪状态。在Linux系统中，对应的系统调用为select/epoll系统调用。
通过该系统调用，一个进程可以监视多个文件描述符（包括socket连接），一旦某个描述符就绪（一般是内核缓冲区可读/可写），内核能够将就绪的状态返回给应用程序。
随后，应用程序根据就绪的状态，进行相应的IO系统调用；目前支持IO多路复用的系统调用，有select、epoll等等。
- select系统调用，几乎在所有的操作系统上都有支持，具有良好的跨平台特性。
- epoll是在Linux 2.6内核中提出的，是select系统调用的Linux增强版本。
在IO多路复用模型中通过select/epoll系统调用，单个应用程序的线程，可以不断地轮询成百上千的socket连接的就绪状态，当某个或者某些socket网络连接有IO就绪状态，就返回这些就绪的状态（或者说就绪事件）。
举个例子来说明IO多路复用模型的流程。发起一个多路复用IO的sys_read读操作的系统调用，流程如下：
![[IO 多路复用模型 流程.png|700]]
（1）选择器注册。在这种模式中，首先，将需要sys_read操作的目标文件描述符（socket连接），提前注册到Linux的select/epoll选择器中，在Java中所对应的选择器类是Selector类。然后，才可以开启整个IO多路复用模型的轮询流程。
（2）就绪状态的轮询。通过选择器的查询方法，查询所有的提前注册过的目标文件描述符（socket连接）的IO就绪状态。通过查询的系统调用，内核会返回一个就绪的socket列表。当任何一个注册过的socket中的数据准备好或者就绪了，就是内核缓冲区有数据了，内核就将该socket加入到就绪的列表中，并且返回就绪事件。
（3）用户线程获得了就绪状态的列表后，根据其中的socket连接，发起sys_read系统调用，用户线程阻塞。内核开始复制数据，将数据从内核缓冲区复制到用户缓冲区。
（4）复制完成后，内核返回结果，用户线程才会解除阻塞的状态，用户线程读取到了数据，继续执行。
本质上，select/epoll系统调用是阻塞式的，属于同步阻塞IO。在读写事件就绪后应用程序的 read 调用也是同步阻塞的，也就是说这个事件的**查询过程**、从**内核缓冲区到用户缓冲区的复制**都是阻塞的。
### 2.5 信号驱动 IO 模型（SIGIO、Signal Driven IO）

在信号驱动IO模型中，用户线程通过向核心注册IO事件的回调函数，来避免IO时间查询的阻塞。
具体的做法是，用户进程预先在内核中设置一个回调函数，当某个事件发生时，内核使用信号（SIGIO）通知进程运行回调函数。 然后用户线程会继续执行，在信号回调函数中调用IO读写操作来进行实际的IO请求操作。
![[信号驱动 IO 模型图例.png|700]]
用户进程通过系统调用，向内核注册SIGIO信号的 owner 进程和以及进程内的回调函数。
内核 IO 事件发生后（比如内核缓冲区数据就位）后，通知用户程序，用户进程通过sys_read系统调用，将数据复制到用户空间，然后执行业务逻辑。
信号驱动IO模型，每当套接字发生IO事件时，系统内核都会向用户进程发送SIGIO事件，所以，一般用于UDP传输，在TCP套接字的开发过程中很少使用，原因是SIGIO信号产生得过于频繁，并且内核发送的SIGIO信号，并没有告诉用户进程发生了什么IO事件。
但是在UDP套接字上，通过SIGIO信号进行下面两个事件的类型判断即可：数据报到达套接字、套接字上发生错误。

举个例子。发起一个异步IO的sys_read读操作的系统调用，流程如下：
（1）设置SIGIO信号的信号处理回调函数。
（2）设置该套接口的属主进程，使得套接字的IO事件发生时，系统能够将SIGIO信号传递给属主进程，也就是当前进程。
（3）开启该套接口的信号驱动I/O机制，通常通过使用fcntl方法的F_SETFL操作命令，使能（enable）套接字的 O_NONBLOCK非阻塞标志和O_ASYNC异步标志完成。
完成以上三步，用户进程就完成了事件回调处理函数的设置。当文件描述符上有事件发生时，SIGIO 的信号处理函数将被触发，然后便可对目标文件描述符执行 I/O 操作。

信号驱动IO优势：用户进程在等待数据时，不会被阻塞，能够提高用户进程的效率。
具体来说：在信号驱动式I/O模型中，应用程序使用套接口进行信号驱动I/O，并安装一个信号处理函数，进程继续运行并不阻塞。

信号驱动IO缺点：
（1）在大量IO事件发生时，可能会由于处理不过来，而导致信号队列溢出。
（2）对于处理UDP套接字来讲，对于信号驱动I/O是有用的。可是，对于TCP而言，由于致使SIGIO信号通知的条件为数众多，进行IO信号进一步区分的成本太高，信号驱动的I/O方式近乎无用。
（3）信号驱动IO可以看成是一种异步IO，可以简单理解为系统进行用户函数的回调。只是，信号驱动IO的异步特性，又做的不彻底。为什么呢？ 信号驱动IO仅仅在IO事件的通知阶段是异步的，而在第二阶段，也就是在将数据从内核缓冲区复制到用户缓冲区这个过程，用户进程是阻塞的、同步的。
### 2.6 异步 IO 模型（Asynchronous IO）

异步IO模型（Asynchronous IO，简称为AIO）。AIO的基本流程是：用户线程通过系统调用，向内核注册某个IO操作。内核在整个IO操作（包括数据准备、数据复制）完成后，通知用户程序，用户执行后续的业务操作。

在异步IO模型中，在整个内核的数据处理过过程中，包括内核将数据从网络物理设备（网卡）读取到内核缓冲区、将内核缓冲区的数据复制到用户缓冲区，**用户程序都不需要阻塞**。
![[异步 IO 模型 图例.png|700]]
（1）当用户线程发起了sys_read系统调用（可以理解为注册一个回调函数），立刻就可以开始去做其他的事，用户线程不阻塞。
（2）内核就开始了IO的第一个阶段：准备数据。等到数据准备好了，内核就会将数据从内核缓冲区复制到用户缓冲区。
（3）内核会给用户线程发送一个信号（Signal），或者回调用户线程注册的回调方法，告诉用户线程，sys_read系统调用已经完成了，数据已经读入到了用户缓冲区。
（4）用户线程读取用户缓冲区的数据，完成后续的业务操作。

异步IO模型的特点：在内核等待数据和复制数据的两个阶段，用户线程都不是阻塞的。
用户线程需要接收内核的IO操作完成的事件，或者用户线程需要注册一个IO操作完成的回调函数。正因为如此，异步IO有的时候也被称为信号驱动IO。
异步IO异步模型的缺点：应用程序仅需要进行事件的注册与接收，其余的工作都留给了操作系统，也就是说，需要底层内核提供支持。

理论上来说，异步IO是真正的异步输入输出，它的吞吐量高于IO多路复用模型的吞吐量。就目前而言，Windows系统下通过IOCP实现了真正的异步IO。
而在Linux系统下，异步IO模型在2.6版本才引入，JDK的对其的支持目前并不完善，因此异步IO在性能上没有明显的优势。

大多数的高并发服务器端的程序，一般都是基于Linux系统的。因而，目前这类高并发网络应用程序的开发，大多采用IO多路复用模型。大名鼎鼎的Netty框架，使用的就是IO多路复用模型，而不是异步IO模型。
## 3 文件描述符

在生产环境Linux系统中，基本上都需要解除文件句柄数的限制。原因是，Linux的系统默认值为1024，也就是说，一个进程最多可以接受1024个socket连接。这是远远不够的。

文件句柄，也叫文件描述符。在Linux系统中一切皆文件，具体可分为：普通文件、目录文件、链接文件和设备文件。文件描述符（File Descriptor）是内核为了高效管理已被打开的文件所创建的索引，它是一个非负整数（通常是小整数），用于指代被打开的文件。所有的IO系统调用，包括socket的读写调用，都是通过文件描述符完成的。

在 Linux 下，通过 `ulimit -m` 可以看到一个进程能够打开的最大文件句柄数量。
当文件句柄不够的时候，就会发出“Socket/File:Can't open so many files”的错误提示。
可以通过 `ulimit -n 1000000` 来设置最大值，但是这个命令是修改对是当前用户环境下的一些基础线程，仅在当前用户环境下有效。
也即是说，在当前的终端工具连接当前shell期间，修改是有效的；一旦断开用户会话，或者说用户退出Linux后，它的数值就又变回系统默认的1024了。并且，系统重启后，句柄数量又会恢复为默认值。

如果想永久地把最大文件描述符数量值保存下来，可以编辑/etc/rc.local开机启动文件，在文件中添加如下内容：
```
ulimit -SHn 1000000
```
以上示例增加-S和-H两个命令选项。选项-S表示软性极限值，-H表示硬性极限值。硬性极限是实际的限制，就是最大可以是100万，不能再多了。软性极限值则是系统发出警告（Warning）的极限值，超过这个极限值，内核会发出警告。

终极解除Linux系统的最大文件打开数量的限制，可以通过编辑Linux的极限配置文件/etc/security/limits.conf来解决，修改此文件，加入如下内容：
* soft nofile 1000000
* hard nofile 1000000

soft nofile表示软性极限，hard nofile表示硬性极限。举个实际例子，在使用和安装目前非常流行的分布式搜索引擎——ElasticSearch时，基本上就必须去修改这个文件，用于增加最大的文件描述符的极限值。当然，在生产环境运行Netty时，最好是修改/etc/security/limits.conf文件，增加文件描述符数量的限制。

除了修改应用进程的文件句柄上限之外，还需要修改内核基本的全局文件句柄上限，通过修改 /etc/sysctl.conf 配置文件来更改，参考的配置如下：
- fs.file-max = 2048000
- fs.nr_open = 1024000
fs.file-max表示系统级别的能够打开的文件句柄的上限，可以理解为全局的句柄数上限。是对整个系统的限制，并不是针对用户的。fs.nr_open指定了单个进程可打开的文件句柄的数量限制，nofile受到这个参数的限制，nofile值不可用超过fs.nr_open值。